\documentclass[10pt]{article}

\usepackage{fullpage}
\usepackage{setspace}
\usepackage{parskip}
\usepackage{titlesec}
\usepackage[section]{placeins}
\usepackage{xcolor}
\usepackage{breakcites}
\usepackage{lineno}
\usepackage{hyphenat}





\PassOptionsToPackage{hyphens}{url}
\usepackage[colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue,
            citecolor = blue,
            anchorcolor = blue]{hyperref}
\usepackage{etoolbox}
\makeatletter

\makeatother


\usepackage{natbib}




\renewenvironment{abstract}
  {{\bfseries\noindent{\abstractname}\par\nobreak}\footnotesize}
  {\bigskip}

\titlespacing{\section}{0pt}{*3}{*1}
\titlespacing{\subsection}{0pt}{*2}{*0.5}
\titlespacing{\subsubsection}{0pt}{*1.5}{0pt}


\usepackage{authblk}


\usepackage{graphicx}
\usepackage[space]{grffile}
\usepackage{latexsym}
\usepackage{textcomp}
\usepackage{longtable}
\usepackage{tabulary}
\usepackage{booktabs,array,multirow}
\usepackage{amsfonts,amsmath,amssymb}
\providecommand\citet{\cite}
\providecommand\citep{\cite}
\providecommand\citealt{\cite}
% You can conditionalize code for latexml or normal latex using this.
\newif\iflatexml\latexmlfalse
\providecommand{\tightlist}{\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}%

\AtBeginDocument{\DeclareGraphicsExtensions{.pdf,.PDF,.eps,.EPS,.png,.PNG,.tif,.TIF,.jpg,.JPG,.jpeg,.JPEG}}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}



\usepackage{float}








\begin{document}

\title{ETF Price Forecaster - AWS ML Engineering Capstone Project}



\author[1]{Marco Sampaio}%
\affil[1]{Affiliation not available}%


\vspace{-1em}




  \date{January 18, 2025}


\begingroup
\let\center\flushleft
\let\endcenter\endflushleft
\maketitle
\endgroup





\selectlanguage{english}
\begin{abstract}
We develop a machine learning model to forecast Exchange Trade Fund
(ETF) indices. We investigate several types of models ranging from
simple baselines to ARIMA and a LightGBM models based on features
extracted from the time series. The best model we find is a LightGBM
model (gain of 6\% over the baseline) which we deploy on AWS as an
interactive dashboard to visualize the forecasts and the historical data
for ETF symbols selected by the user. The dashboard will be available
during the assessment of this report
at~\url{http://dashboard-lb-1124043407.us-east-1.elb.amazonaws.com:8050}~(if
having problems accessing it try this earlier version of the deployment~
at~~\url{http://dashboard-lb-461967644.us-east-1.elb.amazonaws.com:8050/}).%
\end{abstract}%



\sloppy




\tableofcontents

\hypertarget{introduction}{%
\section{Introduction}}

{\label{265150}}

Exchange-Traded Funds (ETFs)~\hyperref[csl:1]{[1]}; \hyperref[csl:2]{[2]} are investment
instruments consisting of a basket of securities--- such as stocks,
bonds, or commodities---designed to track the performance of a specific
index, sector, or asset class. ETFs are traded on stock exchanges,
allowing investors to buy and sell shares throughout the day at market
prices.~

ETFs are an excellent tool for individuals to manage long-term savings
due to their ability to provide diversified exposure to various asset
classes at a low cost \hyperref[csl:3]{[3]}. By investing in ETFs that
track broad market indices, such as the S\&P 500, individuals can
achieve consistent growth over time, benefiting from market trends and
compounding returns at low management fees and tax efficiency, making
them a popular choice for retirement planning and other financial goals.

\hypertarget{problem-statement}{%
\subsection{Problem Statement}}

{\label{635437}}

In this project, we propose to develop an interactive dashboard that
shows the evolution of a selection of long term high performing ETF
indices, together with Machine Learning (ML) model based forecasts (in a
relatively short time window e.g., 1 month). We have in mind the use
case of an individual who:

\begin{itemize}
\tightlist
\item
  has some idea on the domains they want to invest in (e.g., they may be
  thinking about keeping half their savings in an index that tracks the
  overall market, like S\&P 500, and distribute the remainder in the
  tech sector, robotics, health, etc.);
\item
  is interested in regularly investing their savings in such funds
  (e.g., several times a year);
\item
  wants to decide the best time during a period (e.g., one month) to
  buy, if they are investing, or sell, if they are withdrawing their
  funds to use them.
\end{itemize}

Thus, overall, such an individual is not interested in having a detailed
real time forecast of what's happening to the market throughout the day,
but it is interested in the overall trend in a window of a couple of
weeks to make an informed decision on how low they should set their
buying price, or how high their selling price so that their order gets
executed within the time frame they have set while not executing it at a
very unfavorable market fluctuation.

More specifically, the problem can be stated as follows. Given a
historical time series dataset with a set of closing
prices~\(\left\{x_{i,t}\right\}\) for the ETF index~\(i\) on
business day~\(t\) we want to predict the future
prices~\(\left\{\hat{x}_{i, t}\right\}\) for all the indices.~~

\hypertarget{structure-of-the-report}{%
\subsection{Structure of the report}}

{\label{686593}}

This report is split in 4 main parts. In
section~{\ref{162904}} we provide a data exploration
which serves two purposes: i) we select a short list of 100 symbols to
work with while describing the type of investor we are focusing one, and
ii) we explore the properties of the time series to build intuition on
what should be relevant for modeling. In
section~{\ref{158856}} we present the types of models we
will train as well as the evaluation metrics.
Section~{\ref{106179}} consists of the main results
regarding model development, where we conclude on the final model
parameters. Finally, in section~{\ref{615612}} we
describe the engineering of the components needed for the final
deployment in AWS. Our conclusions are summarized in section
{\ref{370266}}.~~

\hypertarget{data-exploration}{%
\section{Data Exploration}}

{\label{162904}}

The first step in addressing any ML problem is to collect the relevant
data, analyse it and clean it.

\hypertarget{etfs-selection}{%
\subsection{ETFs selection~}\label{etfs-selection}}

In our use case we are interested in an investor who is particularly
risk averse so they will want to invest in ETFs that are more main
stream and with a long enough history of high long term returns. Their
goal is to use ETFs as an instrument for long term savings to grow.~
Thus, in this section we start with an analysis of a few statistics
using, for convenience, a comprehensive publicly available Kaggle
dataset~\hyperref[csl:4]{[4]} that has been scrapped from yahoo finance
with thousands of ETF symbols in the US. This dataset already contains
several statistics that describe the characteristics and performance of
the ETF symbols scrapped.

Though the dataset we use for this initial exploratory analysis is
slightly outdated, it serves the purpose of allowing us to iterate fast
and short list a reasonable set of supported symbols, avoiding the
implementation overhead of several statistics used to describe asset
performance. We do not expect this to affect the overall quality of the
solution we will develop later (also because we focus on selecting long
term stable symbols). We select about 100 symbols which should ensure a
reasonable number of mainstream symbols that perform well. A notebook
with the details of this analysis is found in the repo
at~\href{https://github.com/marcoopsampaio/aws_ml_eng_project_stock_prediction/blob/main/experiments/data_exploration/etfs_selection.ipynb}{experiments/data\_exploration/etfs\_selection.ipynb}.~
~\href{https://github.com/marcoopsampaio/aws_ml_eng_project_stock_prediction/blob/main/experiments/data_exploration/etfs_selection.ipynb}{experiments/data\_exploration/etfs\_selection.ipynb}.~

In figure~{\ref{488391}} we show a summary table of the
symbols with the highest annualized returns over 10 years. We also
display their 10 year annualized volatility (stdev\_10y) average daily
volume over 3 months and 10 year sharpe ratio.~ We only show the top
symbols with returns above or equal to the S\&P 500 (SPY), for
simplicity.\selectlanguage{english}
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.56\columnwidth]{figures/screenshot/screenshot}
\caption{{Historical performance for symbols with top 10 year return down to the
S\&P 500 index~
{\label{488391}}%
}}
\end{center}
\end{figure}

We observe that:

\begin{itemize}
\tightlist
\item
  The annualized returns of all these symbols range from 15\% to 53\%.
\item
  A large proportion have a high volume, which means there are plenty of
  mainstream symbols that trade at high volume.
\item
  ~The volatility tends to be larger for larger returns, which is
  expected (higher gains usually imply taking more risk).
\item
  The sharpe ratio, which is a measure of tradeoff between returns and
  volatility is larger than 1 for the majority of symbols and it is
  usually high for symbols with higher return and smaller volatility.~
\end{itemize}

Given these observations, we illustrate two scenarios of an investor who
selects a few symbols to have in their protfolio.

\textbf{Scenario 1:}~We consider an investor that is thinking of a very
long term investment (\textgreater{} 10 years) and is risk averse. They
want to select the symbols with large returns, large sharpe ratio and
that are main stream (large average daily volume). In
figure~{\ref{892879}} we show the selection they get
when applying these criteria.~

\hypertarget{section}{%
\subsection{}\label{section}}\selectlanguage{english}
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.70\columnwidth]{figures/screenshot1/screenshot1}
\caption{{Scenario 1 - High sharpe ratio, 10 year returns \textgreater= SPY, low
stdev (\textless{} 16\%) and large enough volume (\textgreater{} 1\% of
SPY)
{\label{892879}}%
}}
\end{center}
\end{figure}

~\textbf{Scenario 2:~}We consider an investor that is thinking of a
shorter term term investment (\textasciitilde{} 5years) and can take up
a little more risk, while still wanting to select the symbols with large
returns, large sharpe ratio and that are main stream (large average
daily volume). In figure~{\ref{932441}} we show the
selection they get when applying these criteria.\selectlanguage{english}
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.70\columnwidth]{figures/screenshot2/screenshot2}
\caption{{Scenario 2- High sharpe ratio , 5 year returns \textgreater{}= SPY, low
stdev (\textless{} 22\%) and large enough volume (\textgreater{} 1\% of
SPY)
{\label{932441}}%
}}
\end{center}
\end{figure}

In the second scenario we see that a few other symbols with higher
return and risk appear in addition to those in scenario 1.

In the remainder of this report, we choose the top 100 symbols with
highest 10 year returns (i.e., obtained by extending the table of
figure~{\ref{488391}} with a few more symbols below
SPY).~~

\hypertarget{exploratory-time-series-data-analysis}{%
\subsection{Exploratory time series data
analysis}\label{exploratory-time-series-data-analysis}}

In this section we now extract data for the 100 selected symbols up to
the present day, using the yfinance library~\hyperref[csl:5]{[5]}. The
extraction script is found in the
path~\href{https://github.com/marcoopsampaio/aws_ml_eng_project_stock_prediction/blob/main/stock_prediction/etl/ticker_data_extractors.py}{stock\_prediction/etl/ticker\_data\_extractors.py}~of
the repository.

Below, we look into a selection of statistical properties of the dataset
in order to understand the best way of modeling the problem as well as
to cleanup and select the data for modeling.~ The notebook with the full
analysis is found at
\href{https://github.com/marcoopsampaio/aws_ml_eng_project_stock_prediction/blob/main/experiments/data_exploration/data_understanding_and_cleaning.ipynb}{experiments/data\_exploration/data\_understanding\_and\_cleaning.ipynb}.

In figure~{\ref{980651}} we start by observing the
distribution of first timestamps for each of the 100 selected symbols.\selectlanguage{english}
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.98\columnwidth]{figures/output/output}
\caption{{Distribution of first timestamp for the selected symbols
{\label{980651}}%
}}
\end{center}
\end{figure}

We can see that all symbols start more than 13 year ago, so we choose to
clip~ the datasets to~ the period when all symbols have data (which
already gives us more than 13 years of data, up to the present, for
modeling).~

Next, we look at distributions of daily returns. The daily return of the
price time series is defined as~\[r_{t\ }=\frac{x_t-x_{t-1}}{x_{t-1}}\]
where~\(x_{t\ }\)is the closing price at the end of the day at
time~\(t\).

From here on, unless stated otherwise, we focus on the series of daily
returns. The reason to do this is that returns compound over time
resulting in an overall long term exponential growth for the prices so
by working with returns we automatically remove a long term trend.
Furthermore, the daily returns are not sensitive to stock splitting
\hyperref[csl:6]{[6]} (i.e.,~ sometimes the units of an asset are redefined
from one day to the other), which removes the extra complication of
having to adjust for those events.~

In figures~{\ref{578995}}
and~{\ref{257303}} we show, respectively, the
distributions of the mean daily return and the standard deviation over
all symbols for our dataset. ~\selectlanguage{english}
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.98\columnwidth]{figures/output1/output1}
\caption{{Distribution over symbols of the mean of the daily return per symbol in
the selected period
{\label{578995}}%
}}
\end{center}
\end{figure}\selectlanguage{english}
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.98\columnwidth]{figures/output2/output2}
\caption{{Distribution over symbols of the stdev of the daily return per symbol in
the selected period~
{\label{257303}}%
}}
\end{center}
\end{figure}

The main observation is that mean daily returns are peaked around a
small positive value in comparison with the scale of standard
deviations. This is consistent with the fact that the series of daily
returns has a high volatility though in the long run there is a positive
mean effect, thus justifying an overall positive growth in the long run.
This already provides a hint that predicting very precisely the daily
return series will not be possible, though the mean fluctuations of the
series are more likely to be predictable. In any case, for an investor
who is just interested in deciding a selling or buying price to set,
having information on which direction the mean is likely to move and on
how much is already useful to make a decision.

Next, we investigate how the 100 symbols relate to each other by
observing a heatmap for the correlation matrix between any 2 symbols in
figure~{\ref{293994}} (we sort the symbols by
correlation value with SPY).~\selectlanguage{english}
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.98\columnwidth]{figures/output5/output5}
\caption{{Correlation matrix of the time series of daily returns over the selected
symbols
{\label{293994}}%
}}
\end{center}
\end{figure}

We observe that there is a non-trivial distribution of correlations,
with some symbols being highly correlated and others not (see also
figure {\ref{548098}}). From an investors perspective,
this information could be used to diversify away risk in their portfolio
by choosing high performing symbols with low correlation. From a
time-series modeling perspective, this motivates the possibility to
model the time-series jointly instead of in a univariate way. This could
exploit correlations in the data such as ``if symbol 1 is growing,
symbol 2 usually goes down''. Though this is an interesting possibility,
in the experiments, for simplicity, we will focus on univariate
modeling, i.e., we fit one time-series model per symbol.\selectlanguage{english}
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.70\columnwidth]{figures/output4/output4}
\caption{{Distribution of the median of the correlations of a symbol with other
symbols
{\label{548098}}%
}}
\end{center}
\end{figure}

In the following figures we now look more carefully at properties of
each series. In figure~{\ref{273127}} we show a the
evolution of the cumulative daily returns, which is proportional to the
real price.\selectlanguage{english}
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.70\columnwidth]{figures/output3/output3}
\caption{{Relative price evolution for SPY
{\label{273127}}%
}}
\end{center}
\end{figure}

In figure~{\ref{258382}} left panel we can compare it
with the daily returns series. By comparing the two, we clearly see that
by working with daily returns we remove a clear trend. Furthermore, we
see that the daily returns fluctuate heavily around a small positive
value, as already observed when analyzing the standard deviation.\selectlanguage{english}
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.98\columnwidth]{figures/output6/output6}
\caption{{Example of a series of daily returns (left) and auto correlation
analysis (right).
{\label{258382}}%
}}
\end{center}
\end{figure}

In figure~{\ref{258382}}~right panel, we observe and
example of the auto-correlation function~ (ACF) and the partial
auto-correlation functions (pACF) for SPY. These functions encode
information on how patterns in the fluctuations of the series~ repeat
themselves over time and are often used to choose hyperparameters for
model building (see later sections). They are computed between the
series and a lagged version of the series, so in the x axis, the lags
range from 1 day to 252 (the approximate number of trading days in one
year). The 95\% confidence level band helps us understand if the value
of the correlation function is significantly away from zero.~

In figures~{\ref{695023}}
and~{\ref{769583}} we plot the distribution of all lags
with non-zero correlation (i.e., outside the 95\% confidence level band)
over all symbols for the pACF and ACF respectively.\selectlanguage{english}
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.70\columnwidth]{figures/output7/output7}
\caption{{Distribution of the number of non-zero pACF values over the 100 symbols
as a function of the lag (x-axis)
{\label{695023}}%
}}
\end{center}
\end{figure}\selectlanguage{english}
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.70\columnwidth]{figures/output8/output8}
\caption{{Distribution of the number of non-zero ACF values over the 100 symbols
as a function of the lag (x-axis)
{\label{769583}}%
}}
\end{center}
\end{figure}

The ACF and pACF distributions indicate that the correlations are more
pronounced on lags of up to 50 business days (i.e. about 2 months) and
seem to have some smaller peaks at about 100, 150 and 220 business days
(though with progressively lower frequency and correlation function
values). On one hand this can be interpreted as a slight lack of
stationarity (if we also observe the correlation function plots we see
that the functions typically slowly decays to zero) and 3 seasonality
frequencies.

Another measure of the oscillation periods~ of a signal is given by the
Fourier spectrum, which decomposes a signal in sine and cosine signals
of all frequencies (see figure~{\ref{392293}}). We see
that the amplitude of the spectrum has the highest peaks up to about 20
or 30 days, with a heavy tail beyond that.\selectlanguage{english}
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.70\columnwidth]{figures/output9/output9}
\caption{{Fourier spectrum for SPY
{\label{392293}}%
}}
\end{center}
\end{figure}

To finish this section, we look into a very simple statistic that will
both motivate the features we will design for model building as well as
a simple baseline that we will use to compare our models against. In
figure~{\ref{957318}} we show the SPY price series
against a prediction with a shift~\(\delta t\) = 1, 5, 10, 20 and
40 days ahead, obtained given by compounding,~at each
time~\(t\), the average daily return computed with the 20
previous days, on top of the price at time~\(t\).\selectlanguage{english}
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.98\columnwidth]{figures/output10/output10}
\caption{{Rolling geometric averages vs the actual series of daily returns
{\label{957318}}%
}}
\end{center}
\end{figure}

We observe that all curves capture some of the overall trend, with the
shorter time shifts reproducing better the actual signal. This motivates
the usage of rolling averages to capture features of the time series in
model building as well as using a rolling average as a simple model.~~

\section{Methods}

{\label{158856}}

In this section we describe the methods used for model building and for
evaluation. As already mentioned our target is to model the series of
daily returns for each symbol.~

\subsection{Models}

{\label{303985}}

\subsubsection{Baselines}

{\label{344790}}

The simplest benchmark models that can be defined in the context of time
series prediction are:

\begin{itemize}
\tightlist
\item
  \textbf{Predict no change in price:}~Since market prices fluctuate up
  and down the overall mean tends to drift slowly, so we expect this to
  be a good reference baseline (if we do worse than such a simple model
  then we have a clear red flag).
\item
  \textbf{Predict based on a long term average return rate:~}In our use
  case, the investor is mostly interested in the overall trend to ensure
  they invest at a favorable time. The return rate computed, e.g., on a
  few weeks or a few months is typically a good indication of the trend
  of the evolution of the prices so it is expected to provide a decent
  baseline. We already have indications of this from
  figure~{\ref{957318}}. In the results section we will
  tune the window for the rolling average to minimize the prediction
  errors of this simple baseline.
\end{itemize}

Our implementation of these baselines is found at
\href{https://github.com/marcoopsampaio/aws_ml_eng_project_stock_prediction/blob/main/stock_prediction/modeling/baselines.py}{stock\_prediction/modeling/baselines.py}.

\subsubsection{ARIMA models}

{\label{677332}}

These are classic time series models that include autoregressive terms
(linearly regressing on previous series values), moving average terms
(linearly regressing on residual errors) and time series
differencing~\hyperref[csl:7]{[7]}. They are denoted by ARIMA(p,d,q) where
the parameters are non-negative integers such that:

\begin{itemize}
\tightlist
\item
  p is the order (number of time lags),
\item
  d is the degree of differencing, i.e., the number of times the series
  is subtracted by past values to further de-trend the series (in our
  use case we already de-trended the price series once),
\item
  q is the order of the moving average terms.
\end{itemize}

In our exploratory analysis we already~ saw that the dominant periods of
the signal seem to be up to 20 days or 30 days with other longer
subdominant periods. For simplicity, and because running ARIMA models
with much larger orders becomes substantially slower, we will restrict p
and q up to 20. In addition, we use~\(d=1\) to further
de-trend the daily return by the mean value.

Furthermore, we are interested in predicting the value of the time
series n-steps after time~\(t\). This is already supported
by the ARIMA model implementation we use from the statsmodels library
\hyperref[csl:8]{[8]}.

Our implementation of ARIMA models adjusted to our use case is found
at~\href{https://github.com/marcoopsampaio/aws_ml_eng_project_stock_prediction/blob/main/stock_prediction/modeling/arima.py}{stock\_prediction/modeling/arima.py}.

\subsubsection{LightGBM Models}

{\label{690918}}

The ARIMA models above have a very specific set of assumptions regarding
the signal they model so the features of the series that are used are
implicitly constrained the bias of the model. To allow for further
flexibility we also consider using an out of the box model that can
ingest any features we might engineer from the series. We use a gradient
boosting decision trees model provided by the LighGBM
library~\hyperref[csl:9]{[9]} and feed it various features that describe
the history of the time series (statistics, series values etc.) with the
hope of learning additional non-linear dependencies.~

As already mentioned, our target is to predict several time steps after
each time~\(t\). Thus, we expand our dataset for training
the LightGBM model at each time step by repeating n-times each row and
assigning the value of the target to be the daily return 1, \ldots{}, n
days ahead.~

Regarding features, the full set we include for each series is as
follows:

\begin{itemize}
\tightlist
\item
  The values of the series in the last N steps (e.g., 20).
\item
  Rolling window statistics in the last 5, 20, 60, 180, 400 days
  (average, std, min, max) to cover several time scales.
\item
  Time related features to have some dependency on seasonal patterns
  related to the day of the week, month of the year, or part of the
  year. We use, the fraction of the week,~month and year as such
  features.~
\end{itemize}

We use N=20, so we end up with 44 features + label.

Our implementation of UnivariateLightGBM models to fit on our dataset
with 100 symbols is found
at~\href{https://github.com/marcoopsampaio/aws_ml_eng_project_stock_prediction/blob/main/stock_prediction/modeling/lightgbm_model.py}{stock\_prediction/modeling/lightgbm\_model.py}.

\subsubsection{Other approaches}

{\label{633940}}

As already mentioned, we could follow many more modeling approaches as
well as use different algorithms to train our models, namely:

\begin{itemize}
\tightlist
\item
  Modeling the series in a joint way could leverage correlations between
  symbols to extract further signal.
\item
  Adding exogenous information could also be interesting since market
  shifts are often heavily controlled by e.g., public announcements of
  companies performances and other events such as natural disasters,
  political moves etc. just to name a few.
\item
  Non-linear autoregressive~ neural network models~\hyperref[csl:10]{[10]}:
  RNN models such as LSTMs provide another approach to modeling time
  series non-linearities which does not require explicit feature
  engineering. These are typically more difficult to train though.
\end{itemize}

We will leave the investigation of these possibilities for future work,
since the main focus is on the ML engineering side and not on providing
an exhaustive search for the best modeling approach.

\subsection{Evaluation Metrics}

{\label{463309}}

We propose two types of metrics to evaluate the models:

\subsubsection{Loss for training~}

{\label{365893}}

In regression problems, the typical metric
is~\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.root_mean_squared_error.html}{root
mean squared error}~\hyperref[csl:11]{[11]}, which is the average of the
squared residuals of the prediction at each step of the time series. Out
of the box models in standard libraries for regression typically use
this as the default loss for fitting.

\subsubsection{Model evaluation}

{\label{502405}}

To evaluate the quality of the forecasts we propose to focus on Median
Absolute Percentage Error (MedianAPE) for each series - defined as the
Median of the absolute residuals. This will be used in validation to
select~ the best model hyperparameters.~ We choose the median because it
is less sensitive to outlier residuals, which are common to happen in
finance due to isolated unusual market events, which deviate a lot from
the mean behavior and are very unpredictable (so we do not aim to be
able to predict them).~

Furthermore, for each series, we compute the MedianAPE over the n-step
forecast in a very specific way, namely:

\begin{enumerate}
\tightlist
\item
  At each time~\(t+i\)~ with~\(i=1,\ldots,n\) we compute
  the relative error between the forecast price and the actual price
  defined as~~\[\dfrac{\hat{x}_{t+i} - x_{t+i}}{x_{t+i}}\; .\]
\item
  Then we average those relative errors for each time~\(t\)
  and take those as the errors for each time step.
\item
  Finally we take the median over all time steps to obtain our
  definition~ of MedianAPE. We can also take the mean instead of the
  median, which will be more sensitive to large outlier values.
\end{enumerate}

This procedure gives us an aggregate metric describing the performance
of the model, for each symbol, on predicting the next 20 business days.
Since we have 100 symbols, we define a second aggregation of this
distribution of values over symbols to get an overall metric to optimize
in validation in order to in order to select hyperparameters.~

The code used for evaluation is found
at~\href{https://github.com/marcoopsampaio/aws_ml_eng_project_stock_prediction/blob/main/stock_prediction/evaluation/analysis.py}{stock\_prediction/evaluation/analysis.py.}

\section{Model Development}

{\label{106179}}

In this section we describe our approach and results to train the
various models on historical data as well as our evaluation to select
the best training method to deploy our daily retrained dashboard.

\subsection{Data Splitting}

{\label{261812}}

We use a standard temporal train + validation + test splitting strategy.
The train set is used to train each model and the validation set to
evaluate different models to select the best ones. After short listing
one model per model type, we retrain each model on train+validation and
re-evaluate them on the test set. This allows us to observe any temporal
drift effects as well as to check if the validation set conclusions
generalize, thus giving us a more unbiased evaluation (avoiding any
residual risk of over fitting to the validation set ).

The data splitting code is found
\href{https://github.com/marcoopsampaio/aws_ml_eng_project_stock_prediction/blob/main/stock_prediction/etl/ticker_data_extractors.py\#L58}{here.}

\subsection{Hyperparameter Tuning}

{\label{566600}}

In this section we describe the results of hyperparameter tuning of the
various proposed models.~

In figure~{\ref{920430}} we display the variation of
our summary metrics for the rolling average baseline. Though our target
evaluation metrics is the Median of the MedianAPE we also show the
Median of the MeanAPE and the Mean of the MeanAPE over all the symbols.
As expected the errors grow from left to right as the mean is more
sensitive to large outlier values.\selectlanguage{english}
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.70\columnwidth]{figures/screenshot5/screenshot5}
\caption{{Variation of the various performance metrics with the window size in
days, for the rolling average baseline
{\label{920430}}%
}}
\end{center}
\end{figure}

The main conclusion is that the best rolling window is actually quite
large, 400 days, corresponding to about 1.5 years.~

In figure~{\ref{176774}} we display the results for the
ARIMA models for a sweep over 16 configurations of hyperparameters.~\selectlanguage{english}
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.70\columnwidth]{figures/screenshot4/screenshot}
\caption{{Variation of the various performance metrics for the various ARIMA
models as function of the tried hyperparameters~
{\label{176774}}%
}}
\end{center}
\end{figure}

The best model is for (p,q,d) = (0, 1, 5) i.e. a moving average model
with an order of 5, though there are other models with moving average
terms with similar performance. The main conclusion seems to be that not
including moving average terms should not be done, since the performance
degrades considerably in those cases.~~

In figure~{\ref{728131}} we display the results for
LightGBM with various hyperparameter configurations. We chose to vary
the maximum depth of each base tree learner as a way to control the
level of regularization~ (-1 for unbounded) and the learning rate to
control how much each boosting step is adjusting the model (the number
of boosting rounds is fixed to 100).~~\selectlanguage{english}
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.98\columnwidth]{figures/screenshot61/screenshot61}
\caption{{Variation of the various performance metrics for the various LightGBM
models as function of the tried hyperparameter variations
{\label{728131}}%
}}
\end{center}
\end{figure}

Overall the results improve considerably over the other models with the
best model having a Median of MedianAPE of 1.88\% for a maximum depth of
3 and a learning rate of 0.01, i.e., using well regularized trees. We
also indicate in the table other options trading off between a low
median error and mean errors (i.e., reducing the other metrics that are
sensitive to outliers). Note, however, that the differences between the
best model and the alternatives are not large.~

In figure~{\ref{343251}} we show a table summarizing the
performance of the best models for each model type where we also compare
with the simplest baseline where we freeze the daily return rate.

\par\null\selectlanguage{english}
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.84\columnwidth]{figures/screenshot71/screenshot71}
\caption{{Comparison of the best models for each model type (Validation set)
{\label{343251}}%
}}
\end{center}
\end{figure}

All models achieve an improvement larger than 5\% for our target
evaluation metric while for the other metrics the gains are either
smaller or there are no gains. In particular we note that there are some
outlier symbols with larger dominating errors since the gains for Mean
of MeanAPE over the simple baseline are close to null or negative. Also
note that the best model for LightGBM actually performs very negatively
for those alternative metrics, which could indicate some over fitting in
validation.~

The analysis notebooks for this hyperparameter tuning experiments are
found
at~\href{https://github.com/marcoopsampaio/aws_ml_eng_project_stock_prediction/tree/main/experiments/hpt}{experiments/hpt}.

\subsection{Test set Evaluation}

{\label{961135}}

Finally we evaluate the short listed models on the Test set as displayed
in figure~{\ref{585771}}.\selectlanguage{english}
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.84\columnwidth]{figures/screenshot3/screenshot}
\caption{{Comparison of the best models for each model type (Test set)
{\label{585771}}%
}}
\end{center}
\end{figure}

Looking at our target metric, we first observe that the relative gains
over the baseline are smaller, which indicates some over fitting to
validation. Nevertheless, we now see that the best LightGBM model in
validation remains the best in test and much more significantly over the
other models than in validation (6.6\% gain over the baseline versus
2.9\% for the best ARIMA model). It is also reassuring to see that the
performance on the other two metrics is good, actually better than in
validation (this might also be due to temporal drift in the data).

Therefore, we choose to deploy a LightGBM model with a maximum depth of
3 and a learning rate of 0.01, which we will retrain daily on all past
data.

The notebook with this final evaluation is found
at~\href{https://github.com/marcoopsampaio/aws_ml_eng_project_stock_prediction/blob/main/experiments/best_models.ipynb}{experiments/best\_models.ipynb}.

\section{Dashboard App Development}

{\label{615612}}

In this section we describe our implementation of the daily retraining
pipeline to update model forecasts and the dashboard in AWS.~

\subsection{Architecture}

{\label{326741}}

In figure~{\ref{178032}} we provide a high level
diagram of the architecture we developed for the deployment\selectlanguage{english}
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.98\columnwidth]{figures/arch/arch}
\caption{{Overall deployment architecture
{\label{178032}}%
}}
\end{center}
\end{figure}

In our use case forecasts do not need to be updated in real time, since
we are only predicting daily returns, and the model re-training of all
symbols is relatively quick (less than one hour). So we choose to join
the model retraining and inference in one single batch job avoiding
having to serve the model in real time. Thus:

\begin{itemize}
\tightlist
\item
  The model is retrained daily to produce up to date forecasts without
  any manual intervention. We manage this by launching an EC2 instance
  daily, through an EventBridge rule triggering a lambda function, that
  executes the whole data extraction + model training + inference
  pipeline and writes the results to a file in s3. To reduce costs we
  have an additional EventBridge~ rule triggering a lambda function to
  check for stopped EC2 instances every hour and delete them.
\item
  The dashboard app is deployed in an auto scaling group and available
  at a public URL, allowing~ for a user to select the ETFs they want to
  visualize. The dashboard fetches the data directly from S3. It allows
  for the user to clearly see the forecasts and the historical data,~
  zoom specific data periods and consult the specific values of prices
  at specific dates in a table. An example of the final deployed
  dashboard~ can be seen in figure~{\ref{445301}}. The
  live dashboard will remain active during the assessment of this
  project
  at~\url{http://dashboard-lb-1124043407.us-east-1.elb.amazonaws.com:8050}~(if
  having problems accessing it try this earlier version of the
  deployment~
  at~~\url{http://dashboard-lb-461967644.us-east-1.elb.amazonaws.com:8050/}).
\end{itemize}

\par\null\selectlanguage{english}
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.98\columnwidth]{figures/screenshot81/screenshot81}
\caption{{Final deployed dashboard~
{\label{445301}}%
}}
\end{center}
\end{figure}

\hypertarget{lmplementation-details}{%
\subsection{lmplementation Details}}

{\label{673802}}

The implementation of the architecture above involves several
ingredients as described below (see also
the~\href{https://github.com/marcoopsampaio/aws_ml_eng_project_stock_prediction/tree/main/stock_prediction/deployment}{README}
file in the deployment directory of the repository)

\hypertarget{ami-creation-script}{%
\subsubsection{AMI creation script}}

{\label{613034}}

In our architecture, for better control, we choose to launch custom EC2
instances containing the project repository installed with all
dependencies. To avoid wasting compute time pulling the repository and
installing all necessary dependencies every time (which both delays
deployment and can be wasteful in model retraining) we create a custom
AMI. This AMI is used for all instances we launch for different purposes
and contains all scripts that need to be run for model retraining as
well as the script to launch the dashboard app.

The script that fully automates the AMI creation is found
at~\href{https://github.com/marcoopsampaio/aws_ml_eng_project_stock_prediction/blob/main/stock_prediction/deployment/ami_creator.py}{stock\_prediction/deployment/ami\_creator.py}.
The basic logic of the script includes (all done programatically with
boto3):

\begin{itemize}
\tightlist
\item
  Run a "launch-instance" command that launches an instance based on a
  standard ubuntu AMI~and runs a custom bash script that installs all
  dependencies and the repository.
\item
  Run the "make-ami" command (after checking the instance finished the
  installation by connecting to the instance in the AWS console).
\item
  Check that the AMI is available in the AWS console.
\item
  Manually delete the instance in the AWS console to avoid wasting
  resources.
\end{itemize}

This creates a~info.yaml file that contains information on the AMI
created that is used by the other deployment scripts.

Note that for AMI creation we use a t2.micro instance which is one of
the cheapest ones and still has enough compute resources.

\hypertarget{automated-model-retraining-script}{%
\subsubsection{Automated model retraining
script}}

{\label{864816}}

For the model retraining we developed a script to create the resources
which includes setting up triggers to launch an instance for retraining,
managing the associated IAM policies and a trigger to delete shutdown
instances that have completed re-training to avoid extra costs. This is
achieved by using EventBridge rules and AWS Lambda functions.

The script that manages the deployment of the retraining pipeline is
found
at~\href{https://github.com/marcoopsampaio/aws_ml_eng_project_stock_prediction/blob/main/stock_prediction/deployment/daily_retraining_pipeline_deployer.py}{stock\_prediction/deployment/daily\_retraining\_pipeline\_deployer.py}.~
When this deployment script runs it does the following:

\begin{itemize}
\tightlist
\item
  Creates a lambda execution role to allow for access to AWS Lambda,
  EC2, CloudWatch and EventBridge services.
\item
  Then creates lambda functions to launch and delete EC2 instances used
  for model retraining. The lambda functions involve python scripts that
  are responsible either for launching an EC2 instance that runs the
  retraining or checking if there are instances eligible for deletion
  (using the EC2 boto3 client).
\item
  Creates an EventBridge rule that runs daily the lambda that~ runs the
  model retraining daily
\item
  Creates an EventBridge rule that runs hourly the lambda that checks
  for shutdown instances that are eligible for deletion.~
\end{itemize}

The script also contains a function to automatically cleanup the
deployment and all its resources.

Note that the model retraining script that runs when the EC2 instance is
launched by the lambda function is found
at~\href{https://github.com/marcoopsampaio/aws_ml_eng_project_stock_prediction/blob/main/stock_prediction/modeling/train.py}{stock\_prediction/modeling/train.py}.
After the latter runs, the predictions file is copied over to S3 to be
accessible by the dashboard.

For retraining we use t2.small instances since it requires a bit more
memory.

\hypertarget{dashboard-deployment-script}{%
\subsubsection{Dashboard deployment
script}}

{\label{150161}}

The final ingredients are the script to run the dashboard and the
corresponding deployment script.~

We developed a dashboard app python script based on the dash
library~\hyperref[csl:12]{[12]}, which leverages plotly for the plotting. The
script is found
at~\href{https://github.com/marcoopsampaio/aws_ml_eng_project_stock_prediction/blob/main/stock_prediction/dashboard/dashboard.py}{stock\_prediction/dashboard/dashboard.py}~and
it directly reads the required data from S3

As for the script to deploy the dashboard for serving it does the
following (see the script at
\href{https://github.com/marcoopsampaio/aws_ml_eng_project_stock_prediction/blob/main/stock_prediction/deployment/dashboard_deployer.py}{stock\_prediction/deployment/dashboard\_deployer.py}):

\begin{itemize}
\tightlist
\item
  Creates a security group for the dashboard, with the right permission
  to allow for HTTP requests so that the dashboard is accessible via the
  browser from anywhere.
\item
  Creates an IAM profile for the dashboard instance that allows access
  to S3 (to fetch the latest predictions).
\item
  Creates a launch template. This is necessary because we deploy through
  an autoscaling group~ assuming a use case where the dashboard could
  become popular and get spikes in requests, thus requiring spinning up
  more machines.
\item
  Makes a load balancer to distribute the request through the launched
  instances that serve the dashboard.
\item
  Creates the autoscaling group with a minimum of 1 instance and a
  maximum of 3. We use t2.micro instances.
\item
  Creates scale up and down policies and corresponding cloud watch
  alarms to trigger the policies to either add or delete instances.
\end{itemize}

The script also prints out the URL where the dashboard is available
after deployment and if ran with the option "-\/-cleanup" it cleans up
the deployment by killing all machines and deleting all resources.

\hypertarget{conclusion}{%
\section{Conclusion}}

{\label{370266}}

In this project we developed a model to forecast a selection of ETF
prices in a 20 business days window using several ML model types. We
focused on the use case of an investor that is interested in a long term
investment with high returns and low risk.

In data exploration we found that the level of noise of the time series
of daily returns that we aimed to model is quite noisy, but that
predicting the general local trend should be possible. We observed
symbols with various degrees of correlation which motivates further
exploiting it in model building in the future. We also observed self
correlations, which motivated using lags~ of about 20 days in modeling
with ARIMA models, but also observed residual correlations on much
longer periods up to (or even above) 1 year.~

We then explored various types of models, from simple baselines, such as
predicting the last value or compounding a rolling average estimate of
the return,~ to ARIMA and LightGBM based models. After running
hyperparameter tuning, we selected one model from each type and when
evaluating the short listed models in the test set we found a LightGBM
model with the best performance gains over the simplest baseline (about
6\%).~

We then developed code to deploy a dashboard in an autoscaling group to
serve requests and a daily retraining pipeline so that a user consulting
the dashboard can see up to date forecasts for the next 20 days,
together with the historical time series, for the symbols of their
choosing.

\textbf{Note:~}The deployed dashboard will remain available during the
assessment of this report at
\url{http://dashboard-lb-1124043407.us-east-1.elb.amazonaws.com:8050}~(if
having problems accessing it try this earlier version of the deployment~
at~~\url{http://dashboard-lb-461967644.us-east-1.elb.amazonaws.com:8050/})

\par\null

\selectlanguage{english}
\FloatBarrier
\section*{References}\sloppy
\phantomsection
\label{csl:1}[1]``{Exchange-Traded Fund (ETF): How to Invest and What It Is}''. \url{https://www.investopedia.com/terms/e/etf.asp.}

\phantomsection
\label{csl:2}[2]``{Exchange-traded fund - Wikipedia}''. \url{https://en.wikipedia.org/wiki/Exchange-traded_fund.}

\phantomsection
\label{csl:3}[3]``{What are exchange traded funds (ETFs)? | Vanguard}''. \url{https://investor.vanguard.com/investor-resources-education/etfs/what-is-an-etf.}

\phantomsection
\label{csl:4}[4]``{US Funds dataset from Yahoo Finance}''. \url{https://www.kaggle.com/datasets/stefanoleone992/mutual-funds-and-etfs.}

\phantomsection
\label{csl:5}[5]``{yfinance}''. \url{https://pypi.org/project/yfinance/.}

\phantomsection
\label{csl:6}[6]``{What a Stock Split Is, Why Companies Do It, and How It Works, With an Example}''. \url{https://www.investopedia.com/terms/s/stocksplit.asp.}

\phantomsection
\label{csl:7}[7]``{Autoregressive integrated moving average - Wikipedia}''. \url{https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average.}

\phantomsection
\label{csl:8}[8]``{statsmodels.tsa.arima.model.ARIMA - statsmodels 0.14.4}''. \url{https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima.model.ARIMA.html.}

\phantomsection
\label{csl:9}[9]``{lightgbm.LGBMRegressor â€” LightGBM 4.5.0.99 documentation}''. \url{https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html.}

\phantomsection
\label{csl:10}[10]``{Recurrent neural network - Wikipedia}''. \url{https://en.wikipedia.org/wiki/Recurrent_neural_network.}

\phantomsection
\label{csl:11}[11]``{Root mean square deviation - Wikipedia}''. \url{https://en.wikipedia.org/wiki/Root_mean_square_deviation.}

\phantomsection
\label{csl:12}[12]``{Dash Documentation \& User Guide | Plotly}''. \url{https://dash.plotly.com/.}

\end{document}
